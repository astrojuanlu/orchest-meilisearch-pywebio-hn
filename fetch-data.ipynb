{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.130928Z",
     "iopub.status.busy": "2021-12-21T16:39:20.130322Z",
     "iopub.status.idle": "2021-12-21T16:39:20.382429Z",
     "shell.execute_reply": "2021-12-21T16:39:20.381647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import orchest\n",
    "import orchest.transfer\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "import queue\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from lxml.html import fromstring\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.417129Z",
     "iopub.status.busy": "2021-12-21T16:39:20.416515Z",
     "iopub.status.idle": "2021-12-21T16:39:20.421418Z",
     "shell.execute_reply": "2021-12-21T16:39:20.420855Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "usernames = orchest.get_step_param(\"usernames\")\n",
    "all_pages = orchest.get_step_param(\"all_pages\")\n",
    "base_url = \"https://news.ycombinator.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.456487Z",
     "iopub.status.busy": "2021-12-21T16:39:20.455892Z",
     "iopub.status.idle": "2021-12-21T16:39:20.458230Z",
     "shell.execute_reply": "2021-12-21T16:39:20.457707Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "users = [{\"username\": username, \"thread_url\": base_url + \"threads?id=\" + username} for username in usernames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.497114Z",
     "iopub.status.busy": "2021-12-21T16:39:20.496503Z",
     "iopub.status.idle": "2021-12-21T16:39:20.499172Z",
     "shell.execute_reply": "2021-12-21T16:39:20.498660Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "\n",
    "    proxies = set()\n",
    "\n",
    "    for i in parser.xpath(\"//tbody/tr\"):\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \"http://\" + \":\".join(\n",
    "                [i.xpath(\".//td[1]/text()\")[0], i.xpath(\".//td[2]/text()\")[0]]\n",
    "            )\n",
    "            proxies.add(proxy)\n",
    "\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.532244Z",
     "iopub.status.busy": "2021-12-21T16:39:20.531644Z",
     "iopub.status.idle": "2021-12-21T16:39:20.535893Z",
     "shell.execute_reply": "2021-12-21T16:39:20.535375Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Orchest params and input\n",
    "proxy_enabled = orchest.get_step_param(\"proxy_enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.571773Z",
     "iopub.status.busy": "2021-12-21T16:39:20.571164Z",
     "iopub.status.idle": "2021-12-21T16:39:20.654309Z",
     "shell.execute_reply": "2021-12-21T16:39:20.654858Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching with 167 proxies\n"
     ]
    }
   ],
   "source": [
    "if proxy_enabled:\n",
    "    proxies = get_proxies()\n",
    "    proxy_pool = cycle(proxies)\n",
    "    \n",
    "    # At least able to fetch a couple of proxies\n",
    "    assert len(proxies) > 3\n",
    "    print(\"Fetching with %d proxies\" % len(proxies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.694300Z",
     "iopub.status.busy": "2021-12-21T16:39:20.693565Z",
     "iopub.status.idle": "2021-12-21T16:39:20.695627Z",
     "shell.execute_reply": "2021-12-21T16:39:20.696133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def request(url, proxy_enabled, proxy_pool, timeout=2):\n",
    "    kargs = {\"timeout\": timeout}\n",
    "\n",
    "    if proxy_enabled:\n",
    "        proxy = next(proxy_pool)\n",
    "        kargs[\"proxies\"] = {\"http\": proxy, \"https\": proxy}\n",
    "\n",
    "    try:\n",
    "        return requests.get(url, **kargs)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(\"Error: %s [%s]\" % (e, type(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.734065Z",
     "iopub.status.busy": "2021-12-21T16:39:20.733425Z",
     "iopub.status.idle": "2021-12-21T16:39:20.742625Z",
     "shell.execute_reply": "2021-12-21T16:39:20.743238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_till_success(url, proxy_enabled, proxy_pool, tries=25):\n",
    "    for i in range(tries):\n",
    "        resp = request(url, proxy_enabled, proxy_pool)\n",
    "        if resp is None:\n",
    "            pass\n",
    "        elif resp.status_code == 200:\n",
    "            return resp\n",
    "    print(f\"Failed to fetch {url} after {tries} tries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.786362Z",
     "iopub.status.busy": "2021-12-21T16:39:20.785713Z",
     "iopub.status.idle": "2021-12-21T16:39:20.787994Z",
     "shell.execute_reply": "2021-12-21T16:39:20.787422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_comments(soup, username_filter):\n",
    "    \n",
    "    comments = {}\n",
    "\n",
    "    for tr in soup.select(\"tr.comtr\"):\n",
    "        try:\n",
    "            username = tr.select(\".comhead a.hnuser\")[0].text\n",
    "            date = tr.select(\".comhead .age\")[0].get('title')\n",
    "            item_id = tr.get('id')\n",
    "            permalink = base_url + \"item?id=\" + item_id\n",
    "\n",
    "            # Replace <a> text with their href attribute content to avoid collapsed links\n",
    "            for a in tr.select(\".comment\")[0].select(\"a\"):\n",
    "                if type(a.previousSibling) is bs4.element.NavigableString:\n",
    "                    new_string = a.previousSibling.string + a.get('href')\n",
    "                    if type(a.nextSibling) is bs4.element.NavigableString:\n",
    "                        new_string += a.nextSibling.string\n",
    "                        a.nextSibling.extract()\n",
    "                    a.previousSibling.string.replace_with(new_string)\n",
    "                    a.extract()\n",
    "                \n",
    "            body = re.sub(' +', ' ',  tr.select(\".comment\")[0].getText(separator=u'\\n\\n').strip())\n",
    "            if(username == username_filter):\n",
    "                comments[item_id] = {\n",
    "                    \"username\": username,\n",
    "                    \"date\": date,\n",
    "                    \"permalink\": permalink,\n",
    "                    \"body\": body,\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract comment {e}[{type(e)}]\")\n",
    "    \n",
    "    return comments\n",
    "\n",
    "def find_more_link(soup, base_url):\n",
    "    morelink = soup.select(\"a.morelink\")\n",
    "    if len(morelink):\n",
    "        return base_url + morelink[0].get('href')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:20.828764Z",
     "iopub.status.busy": "2021-12-21T16:39:20.828135Z",
     "iopub.status.idle": "2021-12-21T16:39:46.645430Z",
     "shell.execute_reply": "2021-12-21T16:39:46.645926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching users ricklamers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:25<00:00, 25.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:25<00:00, 25.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "comment_store = {}\n",
    "\n",
    "for user in tqdm(users):\n",
    "    \n",
    "    tqdm.write(f\"Fetching users {user['username']}\")\n",
    "    \n",
    "    url_queue = queue.Queue()\n",
    "    url_queue.put(user[\"thread_url\"])\n",
    "    \n",
    "    while not url_queue.empty():\n",
    "        url = url_queue.get()\n",
    "        resp = fetch_till_success(url, proxy_enabled, proxy_pool)\n",
    "        if resp is None:\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(resp.content, features=\"html.parser\")\n",
    "        comment_store.update(find_comments(soup, user[\"username\"]))\n",
    "        \n",
    "        if all_pages:\n",
    "            # Put more link in the queue if it's on the page\n",
    "            more_link = find_more_link(soup, base_url)\n",
    "            if more_link:\n",
    "                url_queue.put(more_link)\n",
    "                print(f\"Adding {more_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T16:39:46.684523Z",
     "iopub.status.busy": "2021-12-21T16:39:46.683868Z",
     "iopub.status.idle": "2021-12-21T16:39:46.687428Z",
     "shell.execute_reply": "2021-12-21T16:39:46.686923Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orchest.transfer.output_to_disk(comment_store, name=\"comments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "orchest-kernel-b0bd58a5-5b71-4078-879c-57fcaa540579"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
